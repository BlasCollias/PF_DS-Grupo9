{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOPIC MODELLING PARA review_comments , separados por review_score\n",
    "\n",
    "Tipo de modelo de ML UNSUPERVISED\n",
    "\n",
    "VERSION 2, CON SUBCLASE DE COUNTVECTORIZER, CON STEMMER INLCUIDO, Y LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/moncho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#\n",
    "import nltk\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import Stemmer\n",
    "#Para traducir la salida: \n",
    "from deep_translator import GoogleTranslator\n",
    "#import matplotlib as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#hay varios q no se usan, del otro archivo. limpiar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos los datos que usamos:\n",
    "Review = pd.read_csv('Datasets/olist_order_reviews_dataset.csv')\n",
    "\n",
    "# tiro nulos: \n",
    "Review.dropna(subset = 'review_comment_message', inplace= True)\n",
    "\n",
    "# me quedo solo con estas cols: TMB AGREGO FECHA A VER SI SE PUEDE PLOTTEAR\n",
    "Review = Review[['review_score', 'review_comment_message', 'review_id','review_creation_date']]\n",
    "\n",
    "# los separo por score, para poder sacarle los 'topic' por separado a cada uno.\n",
    "estrella_1 = Review[Review['review_score'] == 1]\n",
    "estrella_2 = Review[Review['review_score'] == 2]\n",
    "estrella_3 = Review[Review['review_score'] == 3]\n",
    "estrella_4 = Review[Review['review_score'] == 4]\n",
    "estrella_5 = Review[Review['review_score'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DEFINO LA SUB CLASE DE CountVectorizer:  \n",
    "\n",
    "stemmer = Stemmer.Stemmer('pt')  #import Stemmer , ya lo hice en otra celda. \n",
    "\n",
    "#stemmer=RSLPStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "# Override TfidfVectorizer\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    '''def Busco_palabra_raiz(texto): #esta esta era la funcion nuestra del ETL del modelo classifier. \n",
    "        stemmer=RSLPStemmer()\n",
    "        return [stemmer.stem(c) for c in texto.split()]'''\n",
    "    def build_analyzer(self):   #pruebo con esta a ver si me hace un mejor analisis. \n",
    "        analyzer = super(CountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: stemmer.stemWords(analyzer(doc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords en portugues: EDITAR SEGUN OUTPUTS. \n",
    "\n",
    "\n",
    "stoplist = stopwords.words('portuguese')\n",
    "#le agrego algunas: \n",
    "stoplist_ampliada = stopwords.words('portuguese') + ['produto', 'satisfeita', 'n','recomendo', 'recom']\n",
    "# VER: agregar el 'aun' (en pt)?\n",
    "\n",
    "#veo de quitarle el 'no', asi arma mas frases con sentido de negacion...\n",
    "stoplist_ampliada.remove('não')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#por las dudas q se haya transformado lo instacio de nuevo los datos: \n",
    "comments1=estrella_1['review_comment_message'] # ie, todos los comentarios de review_score==1. \n",
    "\n",
    "# Sacar non-word characters, so numbers and ___ etc: VER SI HACE FALTA ESTA LIMPIEZA. \n",
    "#comments1 = comments1.str.replace(\"[^A-Za-z ]\", \" \")\n",
    "\n",
    "#el vectorizador con sus parametros, ACA ES DONDE SE PUEDE JUGAR: \n",
    "# Document frequency: min_df=5 (debe aparecer al menos 5 veces), max_df=0.5 (debe aparecer en menos de la mitad)\n",
    "# agregar max_features=500????\n",
    "vectorizer = StemmedCountVectorizer(stop_words=stoplist_ampliada, ngram_range=(2,4), min_df=5, max_df=0.5)\n",
    "matrix = vectorizer.fit_transform(comments1)\n",
    "\n",
    "#convierte a df la matriz vectorizda. \n",
    "words_df = pd.DataFrame(matrix.toarray(),\n",
    "                        columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moncho/Library/Python/3.9/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: no entregado, ningún momento, no me gusta\n",
      "Topic 1: no recibí, todavía en, no llegues\n",
      "Topic 2: no recibí, nota fiscal, voltios de dinero\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Uso LDA: \n",
    "# cuantos topics: \n",
    "n_topics = 3\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=n_topics)\n",
    "# fit con el output de la celda anterior:\n",
    "model.fit(matrix)\n",
    "\n",
    "# cuantas words / topic (incluyendo los ngrams...):\n",
    "n_words = 3\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "#TRADUCTOR: instancio uno, si no lo hubiera:\n",
    "traductor = GoogleTranslator(source='pt', target='es')\n",
    "\n",
    "topic_list = []\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    top_n = [traductor.translate(feature_names[i])   #traductor.translate(feature_names[i])   #lo traduzco asi entiendo...\n",
    "             for i in topic.argsort()\n",
    "             [-n_words:]][::-1]\n",
    "    top_features = ', '.join(top_n) #separo por comas asi se identifican los ngrams de las palabras sueltas.\n",
    "    topic_list.append(f\"topic_{'_'.join(top_n[:3])}\") \n",
    "\n",
    "    print(f\"Topic {topic_idx}: {top_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output con (2,4), max_df=0.5  \n",
    "'''Topic 0: nota fiscal, acabo de recibir, voltios de dinero\n",
    "Topic 1: no llegues, todavía en, no ha llegado aún\n",
    "Topic 2: no recibí, no entregado, no recomiendo\n",
    "Topic 3: no recibí, todavía en, no recibido aún\n",
    "Topic 4: no entregado, todavía en, el tiempo de entrega\n",
    "Topic 5: No puedo, no funciona, no compre'''\n",
    "# idem pero le saque el recom o recomendo...\n",
    "'''Topic 0: no recibí, todavía en, no recibido aún\n",
    "Topic 1: no recibí, no recomiendo, voltios de dinero\n",
    "Topic 2: no recibí, no recibí, ningún momento\n",
    "Topic 3: no funciona, no me gusta, vino defectuoso\n",
    "Topic 4: acabo de recibir, comprado en, nota fiscal\n",
    "Topic 5: no llegues, no entregado, todavía en'''\n",
    "#(3,4), 5 topic, 1 palabra. \n",
    "'''Topic 0: no ha llegado aún\n",
    "Topic 1: no recibido aún\n",
    "Topic 2: no recibí\n",
    "Topic 3: no recibido aún\n",
    "Topic 4: ahora no recibo'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2,4  10 topic 1 word\n",
    "Topic 0: no recibí\n",
    "Topic 1: no recibí\n",
    "Topic 2: no recibí\n",
    "Topic 3: nota fiscal\n",
    "Topic 4: acabo de recibir\n",
    "Topic 5: no es original\n",
    "Topic 6: no recibí\n",
    "Topic 7: voltios de dinero\n",
    "Topic 8: no me gusta\n",
    "Topic 9: no entregado\n",
    "\n",
    "# idem pero 3,4  \n",
    "Topic 0: no recibido aún\n",
    "Topic 1: No hasta ahora\n",
    "Topic 2: momento no recibido\n",
    "Topic 3: no recibido aún\n",
    "Topic 4: ahora no recibo\n",
    "Topic 5: no recibi la compra\n",
    "Topic 6: aun no lo he recibido\n",
    "Topic 7: no ha llegado aún\n",
    "Topic 8: no obtuve ninguno\n",
    "Topic 9: no recibí\n",
    "\n",
    "#idem pero 10 words, 1 topic. \n",
    "Topic 0: no recibido aún, no ha llegado aún, ahora no recibo, momento no recibido, todavía no entregado, quiero dinero de vuelta, no recibí, aun no lo he recibido, no recibido aún, no obtuve ninguno\n",
    "\n",
    "# (2,4), 3, 3. (comments1)\n",
    "Topic 0: no entregado, ningún momento, no me gusta\n",
    "Topic 1: no recibí, todavía en, no llegues\n",
    "Topic 2: no recibí, nota fiscal, voltios de dinero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-10-02 00:00:00'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Review['review_creation_date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-08-31 00:00:00'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Review['review_creation_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUEBO CON LAS DE ULTIMOS 2 MESES. \n",
    "\n",
    "estrella_1_ultimos = Review[(Review['review_score'] == 1) & ( Review['review_creation_date']>'2018-06-31 00:00:00')]\n",
    "#estrella_2 = Review[Review['review_score'] == 2]\n",
    "#estrella_3 = Review[Review['review_score'] == 3]\n",
    "#estrella_4 = Review[Review['review_score'] == 4]\n",
    "estrella_5_ultimos = Review[(Review['review_score'] == 5) & ( Review['review_creation_date']>'2018-06-31 00:00:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moncho/Library/Python/3.9/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: entrega rápida\n",
      "Topic 1: antes de la fecha límite\n",
      "Topic 2: bien antes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#comments1_ultimos=estrella_1_ultimos['review_comment_message'] # me quedo con los comentarios. \n",
    "comments5_ultimos=estrella_5_ultimos['review_comment_message'] # me quedo con los comentarios. \n",
    "\n",
    "# Sacar non-word characters, so numbers and ___ etc: VER SI HACE FALTA ESTA LIMPIEZA. \n",
    "#comments1 = comments1.str.replace(\"[^A-Za-z ]\", \" \")\n",
    "\n",
    "#el vectorizador con sus parametros, ACA ES DONDE SE PUEDE JUGAR: \n",
    "# Document frequency: min_df=5 (debe aparecer al menos 5 veces), max_df=0.5 (debe aparecer en menos de la mitad)\n",
    "# agregar max_features=500????\n",
    "vectorizer = StemmedCountVectorizer(stop_words=stoplist_ampliada, ngram_range=(2,4), min_df=5, max_df=0.5)\n",
    "matrix = vectorizer.fit_transform(comments5_ultimos)\n",
    "\n",
    "#convierte a df la matriz vectorizda. \n",
    "words_df = pd.DataFrame(matrix.toarray(),\n",
    "                        columns=vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "# Uso LDA: \n",
    "# cuantos topics: \n",
    "n_topics = 3\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=n_topics)\n",
    "# fit con el output de la celda anterior:\n",
    "model.fit(matrix)\n",
    "\n",
    "# cuantas words / topic (incluyendo los ngrams...):\n",
    "n_words = 1\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "#TRADUCTOR: instancio uno, si no lo hubiera:\n",
    "traductor = GoogleTranslator(source='pt', target='es')\n",
    "\n",
    "topic_list = []\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    top_n = [traductor.translate(feature_names[i])   #traductor.translate(feature_names[i])   #lo traduzco asi entiendo...\n",
    "             for i in topic.argsort()\n",
    "             [-n_words:]][::-1]\n",
    "    top_features = ', '.join(top_n) #separo por comas asi se identifican los ngrams de las palabras sueltas.\n",
    "    topic_list.append(f\"topic_{'_'.join(top_n[:3])}\") \n",
    "\n",
    "    print(f\"Topic {topic_idx}: {top_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARA comments1_ultimo, (2,4), 3,3. \n",
    "Topic 0: no recibí, no vino, no funciona\n",
    "Topic 1: no entregado, no recibí, no recibí\n",
    "Topic 2: todavía en, no llegues, No puedo\n",
    "Para comments5_ultimo: \n",
    "Topic 0: antes de la fecha límite, entregado, a tiempo\n",
    "Topic 1: ha llegado, antes de la fecha límite, llegó antes de la fecha límite\n",
    "Topic 2: antes de la fecha límite, bien antes, mucho antes de la fecha límite\n",
    "\n",
    "OSEA, NO CAAMBIA MUCHO, INCLUSO SI ANALIZAMOS PARA ULTIMOS DOS MESES, LOS TOPICS SIGUEN SIENDO LOS MISMOS.!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
